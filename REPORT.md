# MNIST手写数字分类实验报告

## 1. 实验目的
利用Python实现多种分类算法，并在MNIST数据集上进行对比分析，研究不同降维方法和分类器的性能表现。

## 2. 实验数据
- **数据集**: MNIST手写数字数据集
- **数据规模**: 70,000个样本（训练集56,000，测试集14,000）
- **特征维度**: 784维（28×28像素图像）
- **类别数**: 10（数字0-9）

## 3. 实验方法

### 3.1 降维方法

#### 3.1.1 主成分分析（PCA）
PCA是一种无监督的线性降维方法，通过特征值分解寻找数据方差最大的方向。

**实现原理**:
1. 数据中心化
2. 计算协方差矩阵
3. 特征值分解
4. 选取前k个主成分

**优点**:
- 保留数据的主要变化方向
- 降维后数据仍为线性组合
- 计算效率高

#### 3.1.2 线性判别分析（LDA）
LDA是一种有监督的线性降维方法，寻找最大化类间距离同时最小化类内距离的投影方向。

**实现原理**:
1. 计算类内散度矩阵和类间散度矩阵
2. 求解广义特征值问题
3. 选取前k个判别方向（最多k=c-1，c为类别数）

**优点**:
- 利用类别信息
- 优化类别可分性
- 对分类任务更有针对性

**限制**:
- 对于10类问题，最多只能降到9维

### 3.2 分类算法

#### 3.2.1 二次判别函数（QDF）

QDF基于贝叶斯决策理论，假设每类数据服从高斯分布。

**基本形式**:
```
g_i(x) = -0.5 * log|Σ_i| - 0.5 * (x-μ_i)^T * Σ_i^(-1) * (x-μ_i) + log(P_i)
```

其中：
- μ_i: 第i类的均值向量
- Σ_i: 第i类的协方差矩阵
- P_i: 第i类的先验概率

**正则化方法**:

##### (1) 正则化判别分析（RDA）
通过两个参数α和β对协方差矩阵进行正则化：
```
Σ_i = α * Σ_i + (1-α) * Σ_pooled
Σ_i = β * Σ_i + (1-β) * (trace(Σ_i)/d) * I
```

**作用**:
- α参数：控制向共享协方差收缩
- β参数：控制向对角矩阵收缩
- 防止协方差矩阵奇异
- 提高泛化能力

##### (2) 改进二次判别函数（MQDF）
只保留协方差矩阵的前k个主特征值和特征向量：
```
Σ_i ≈ Φ_k * Λ_k * Φ_k^T + δ * I
```

**优点**:
- 降低计算复杂度
- 减少参数数量
- 避免小特征值导致的数值问题

#### 3.2.2 K近邻（KNN）分类器

KNN是一种基于实例的学习方法，通过k个最近邻的投票决定类别。

**加速策略 - KD树**:
- 将训练数据组织成树结构
- 快速查找最近邻
- 时间复杂度从O(n)降到O(log n)

**优点**:
- 原理简单直观
- 无需训练过程
- 对数据分布无假设

**缺点**:
- 对维度诅咒敏感
- 预测时需要存储所有训练数据

## 4. 实验设置

### 4.1 实验参数
- **测试维度**: [10, 20, 30, 50, 100, 200]
- **样本数量**: 10,000（为加速实验）
- **QDF参数**:
  - RDA: α=0.5, β=0.5
  - MQDF: k=dim/2
- **KNN参数**: k=5, 使用KD树加速

### 4.2 评价指标
- **准确率（Accuracy）**: 正确分类样本数/总样本数
- **训练时间**: 模型训练所需时间（秒）

## 5. 实验结果

### 5.1 准确率对比

实验在不同降维维度下测试了6种方法组合：
1. PCA + QDF(RDA)
2. PCA + QDF(MQDF)
3. PCA + KNN
4. LDA + QDF(RDA)
5. LDA + QDF(MQDF)
6. LDA + KNN

结果显示：
- 随着维度增加，准确率总体呈上升趋势
- PCA降维在较高维度（100-200）时表现更好
- LDA降维在低维度下已能达到较高准确率
- KNN分类器通常表现最佳

### 5.2 训练时间对比

- QDF方法的训练时间随维度增加而增加（需要计算协方差矩阵）
- KNN的训练时间相对稳定（主要是建立KD树）
- MQDF比RDA略快（减少了特征维度）

## 6. 结果分析与见解

### 6.1 降维方法对比

**PCA vs LDA**:

1. **性能差异**:
   - LDA在低维度下性能优于PCA（利用了类别信息）
   - PCA在高维度下有更大的提升空间
   - LDA受限于最多9个判别方向

2. **适用场景**:
   - 当维度限制较严格时，LDA更合适
   - 当需要保留更多信息时，PCA更灵活
   - LDA对分类任务优化更直接

### 6.2 分类器对比

**QDF vs KNN**:

1. **准确率**:
   - KNN通常表现最佳
   - QDF(RDA)和QDF(MQDF)性能相近
   - MQDF在高维时可能略优于RDA

2. **计算效率**:
   - QDF训练慢但预测快
   - KNN训练快但预测需要遍历数据
   - KD树加速了KNN的预测

3. **模型假设**:
   - QDF假设高斯分布，可能不完全符合数据
   - KNN无参数假设，更灵活但需要更多数据

### 6.3 维度选择建议

根据实验结果：
- **极低维度（≤20）**: 建议使用LDA + KNN
- **中等维度（30-50）**: PCA或LDA + KNN均可
- **高维度（≥100）**: PCA + KNN表现最佳

### 6.4 改进方向

1. **降维方法改进**:
   - 尝试核PCA（KPCA）捕捉非线性关系
   - 探索其他降维方法如t-SNE、UMAP

2. **分类器改进**:
   - QDF可以尝试其他正则化参数
   - KNN可以使用加权投票（距离加权）
   - 考虑集成学习方法

3. **特征工程**:
   - 提取边缘、纹理等高层特征
   - 使用深度学习提取特征

## 7. 结论

本实验在MNIST数据集上对比了不同降维方法和分类器的组合：

1. **降维方法**: LDA在低维度下更有效，PCA在高维度下更灵活
2. **分类器**: KNN表现最好，QDF在低维度下也有不错表现
3. **维度影响**: 适当增加维度能提升性能，但存在边际递减效应
4. **计算效率**: 需要在准确率和时间之间权衡

实验验证了不同方法的特点和适用场景，为实际应用提供了参考依据。
